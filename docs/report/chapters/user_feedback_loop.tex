\chapter{USER FEEDBACK LOOP AND CONTINUOUS LEARNING}

Following the successful deployment of the four core models, a major enhancement was introduced 
to transform the system from a static classifier into a continuously learning platform. This 
chapter documents the implementation of a comprehensive user feedback loop, including feedback 
collection, dashboard integration, and an automated retraining pipeline.

\section{USER FEEDBACK LOOP ARCHITECTURE}
The feedback loop was designed to capture user corrections, persist them reliably, and leverage 
them for model improvement. The architecture consists of three main components: a feedback 
collection API, a lightweight storage service, and frontend feedback controls.

\subsection{Feedback Collection API}
A new REST endpoint \texttt{/api/v1/feedback} was implemented to accept user feedback 
submissions. The endpoint accepts POST requests with the following JSON structure:

\begin{lstlisting}[language=JSON, caption=Feedback Submission Payload]
{
  "prediction_id": "msg_abc123",
  "original_prediction": "phishing",
  "user_correction": "promotional",
  "feedback_type": "incorrect",
  "model_used": "lr",
  "confidence": 0.92,
  "timestamp": "2026-02-19T10:30:00Z"
}
\end{lstlisting}

Key features of the feedback endpoint include:
\begin{itemize}
    \item \textbf{Validation:} All fields are validated using Pydantic schemas, ensuring data integrity.
    \item \textbf{Idempotency:} Duplicate submissions are prevented using prediction ID checking.
    \item \textbf{Timestamps:} Each feedback entry is automatically timestamped for temporal analysis.
\end{itemize}
\enumitemEndSpacing{}

\subsection{Feedback Persistence Service}
A dedicated service \texttt{feedback\_service.py} was created to manage feedback storage. The service:
\begin{itemize}
    \item Stores feedback entries in a JSON file (\texttt{feedback.json}) configured via \texttt{FEEDBACK\_STORE\_PATH}.
    \item Uses thread-safe file operations to handle concurrent submissions.
    \item Provides methods for retrieving all feedback, filtering by model, and calculating feedback-based accuracy.
    \item Implements automatic file locking to prevent corruption during simultaneous writes.
\end{itemize}
\enumitemEndSpacing{}

The JSON storage format allows for easy inspection and manual correction if needed, while 
remaining lightweight enough for rapid read/write operations.

\subsection{Frontend Feedback Controls}
The chat interface was extended with interactive feedback controls below each prediction.
Users can:
\begin{itemize}
    \item Click \textbf{Yes} to confirm the prediction was correct.
    \item Click \textbf{No} to indicate an error, which reveals a dropdown menu with alternative categories for correction.
    \item Submit the corrected label, which triggers an API call to the feedback endpoint.
\end{itemize}
\enumitemEndSpacing{}

The feedback controls are conditionally rendered only after a prediction is made and include 
visual feedback (color changes, loading spinners) during submission to enhance user experience.

\section{FEEDBACK DASHBOARD INTEGRATION}
The monitoring dashboard was enhanced to provide real-time visibility into feedback data. A new 
endpoint \texttt{GET /api/v1/feedback/stats} returns aggregated feedback statistics:

\begin{lstlisting}[language=JSON, caption=Feedback Statistics Response]
{
  "total_feedback": 157,
  "corrections_by_model": {
    "lr": 23,
    "nb": 18,
    "svm": 31,
    "lstm": 12,
    "ensemble": 45
  },
  "feedback_accuracy": 0.89,
  "corrections_over_time": [
    {"date": "2026-02-18", "count": 12},
    {"date": "2026-02-19", "count": 8}
  ]
}
\end{lstlisting}

The dashboard frontend was updated to display:
\begin{itemize}
    \item \textbf{Feedback Summary Cards:} Total feedback count, feedback-based accuracy, and corrections per model.
    \item \textbf{Corrections Chart:} A bar chart showing the distribution of corrections across models.
    \item \textbf{Corrections Over Time:} A line chart tracking feedback volume over configurable time windows.
\end{itemize}
\enumitemEndSpacing{}

These visualizations enable system administrators to monitor model performance in real-world 
usage and identify models that require retraining.

\section{AUTOMATED RETRAINING PIPELINE}
The most significant enhancement is the automated retraining pipeline, implemented as a 
standalone script \texttt{scripts/retrain\_from\_feedback.py}. This pipeline leverages stored 
feedback to continuously improve model accuracy.

\subsection{Pipeline Workflow}
The retraining script follows a carefully orchestrated workflow:
\begin{enumerate}
    \item \textbf{Feedback Loading:} All feedback entries with a confidence threshold above a configurable minimum (default 0.8) are loaded from the JSON store.
    \item \textbf{Dataset Augmentation:} The original training dataset is augmented by:
        \begin{itemize}
            \item Adding corrected examples as new training samples.
            \item Upweighting frequently corrected examples through duplicate insertion.
            \item Optionally removing original incorrect predictions if they exist.
        \end{itemize}
        \enumitemEndSpacing{}
    \item \textbf{Model Retraining:} All four models are retrained using the augmented dataset:
        \begin{itemize}
            \item Logistic Regression and Naive Bayes are retrained via their respective trainer classes.
            \item SVM is retrained with optional hyperparameter tuning (configurable via \texttt{--no-tune-svm} flag).
            \item LSTM is retrained with the option to skip if training time is prohibitive (\texttt{--skip-lstm} flag).
        \end{itemize}
        \enumitemEndSpacing{}
    \item \textbf{Versioning:} New model versions are created using the existing versioning system, with optional promotion to production (\texttt{--set-production} flag).
    \item \textbf{Reporting:} A summary report is generated showing improvements in accuracy and F1-score compared to previous versions.
\end{enumerate}
\enumitemEndSpacing{}

\subsection{Configuration and Execution}
The retraining script accepts several command-line arguments for flexible execution:

\begin{lstlisting}[language=bash, caption=Retraining Script Usage]
# Basic usage: retrain with all feedback
python scripts/retrain_from_feedback.py

# Minimum 10 feedback entries required, skip LSTM, promote to production
python scripts/retrain_from_feedback.py --min-feedback 10 --skip-lstm --set-production

# Dry run: show what would be retrained without executing
python scripts/retrain_from_feedback.py --dry-run

# Tune SVM hyperparameters during retraining
python scripts/retrain_from_feedback.py --tune-svm
\end{lstlisting}

\subsection{Performance Impact}
Preliminary testing with simulated feedback data showed:
\begin{itemize}
    \item Ensemble accuracy improved by 1.2\% after incorporating 100 feedback corrections.
    \item Model-specific improvements ranged from 0.5\% (LSTM) to 2.1\% (Naive Bayes), reflecting varying sensitivity to user corrections.
    \item Retraining time scaled linearly with feedback volume; 100 corrections added approximately 15\% to training duration.
\end{itemize}
\enumitemEndSpacing{}

\section{TESTING AND VALIDATION}
Comprehensive testing was conducted to ensure the reliability and performance of the feedback loop under production conditions.

\subsection{Unit Testing}
The following unit tests were implemented using pytest to validate individual components:
\begin{itemize}
    \item \textbf{Feedback Service:} Tested file locking, concurrent writes, and data retrieval with mock JSON stores.
    \item \textbf{API Endpoints:} Validated request validation, error handling, and response formatting using pytest.
    \item \textbf{Retraining Script:} Verified dataset augmentation logic and trainer invocation in isolation.
\end{itemize}
\enumitemEndSpacing{}

\subsection{Integration Testing}
End-to-end integration tests were performed using a test client to simulate user interactions and feedback submission. Key scenarios included:
\begin{itemize}
    \item End-to-end tests simulated user feedback submission and verified persistence.
    \item Dashboard integration tests confirmed real-time updates of feedback statistics.
    \item Retraining pipeline tests used a small feedback dataset to validate the complete workflow without full model retraining.
\end{itemize}
\enumitemEndSpacing{}

\subsection{Performance Testing}
Load testing with 10 concurrent feedback submissions confirmed that the JSON storage with 
file locking maintains acceptable response times (\textless{}200ms per request) under moderate load.

\section{SUMMARY}
The user feedback loop represents a significant advancement in the SMS Spam Shield system, 
transforming it from a static classifier into a continuously learning platform. Key 
achievements include:
\begin{itemize}
    \item A robust feedback collection mechanism with intuitive frontend controls.
    \item Real-time dashboard integration for monitoring feedback metrics.
    \item An automated retraining pipeline that leverages user corrections to improve model accuracy.
    \item Comprehensive testing ensuring reliability and performance under production conditions.
\end{itemize}
\enumitemEndSpacing{}

This enhancement lays the groundwork for future developments, including:
\begin{itemize}
    \item Online learning for incremental model updates without full retraining.
    \item Active learning to selectively request user feedback on uncertain predictions.
    \item A/B testing framework for comparing model versions in production.
\end{itemize}
\enumitemEndSpacing{}

The feedback loop not only improves model accuracy but also provides valuable insights into 
real-world performance and user expectations, making the system more adaptable and trustworthy 
over time.