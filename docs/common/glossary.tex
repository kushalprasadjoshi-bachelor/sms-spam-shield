% ------------------------- GLOSSARY ------------------------------
\chapter*{GLOSSARY}
\addcontentsline{toc}{chapter}{GLOSSARY}%
\label{glossary}

\textbf{Short Message Service (SMS):}  
A text messaging service component of mobile communication systems used for exchanging short 
text messages between mobile devices~\cite{almeida2011contributions}.

\textbf{Spam:}  
Unsolicited or unwanted messages sent in bulk, often for advertising, fraudulent, or malicious 
purposes~\cite{almeida2011contributions}.

\textbf{Phishing:}  
A form of cyberattack in which deceptive messages attempt to obtain sensitive information such 
as passwords, banking details, or personal data~\cite{aibook2014}.

\textbf{Machine Learning (ML):}  
A field of artificial intelligence that enables systems to learn patterns from data and make 
predictions without explicit programming~\cite{aibook2014}.

\textbf{Binary Classification:}  
A classification task in which input data is assigned to one of two possible classes, such as 
spam or non-spam.

\textbf{Multi-Category Classification:}  
A classification task in which input data is assigned to one of several predefined categories 
rather than a binary decision~\cite{aibook2006}.

\textbf{Explainable Artificial Intelligence (XAI):}  
Methods and techniques that make the predictions and internal behavior of artificial intelligence 
models understandable to humans~\cite{ribeiro2016why}.

\textbf{Black-Box Model:}  
A machine learning model whose internal decision-making process is not directly interpretable 
or transparent to human users.

\textbf{Logistic Regression:}  
A supervised machine learning algorithm used for classification that estimates class 
probabilities using a logistic function~\cite{aibook2006}.

\textbf{Naive Bayes Classifier:}  
A probabilistic machine learning algorithm based on Bayesâ€™ theorem with an assumption of 
conditional independence among features~\cite{mcCallum1998comparison}.

\textbf{Support Vector Machine (SVM):}  
A margin-based supervised learning algorithm that separates data points using an optimal 
hyperplane in a high-dimensional feature space~\cite{cortes1995support}.

\textbf{Recurrent Neural Network (RNN):}  
A neural network architecture designed for sequential data processing by maintaining internal 
memory states across time steps~\cite{aibook2014}.

\textbf{Long Short-Term Memory (LSTM):}  
A specialized type of recurrent neural network capable of learning long-range dependencies by 
mitigating the vanishing gradient problem~\cite{hochreiter1997long}.

\textbf{Ensemble Learning:}  
A machine learning technique that combines predictions from multiple models to improve accuracy, 
robustness, and generalization~\cite{dietterich2000ensemble}.

\textbf{Local Interpretable Model-Agnostic Explanations (LIME):}  
An explainability technique that explains individual predictions by approximating the model 
locally using an interpretable surrogate model~\cite{ribeiro2016why}.

\textbf{SHapley Additive exPlanations (SHAP):}  
A unified framework for model interpretation based on cooperative game theory that assigns 
contribution values to individual features~\cite{lundberg2017unified}.

\textbf{Software Development Life Cycle (SDLC):}  
A structured process for planning, designing, developing, testing, and maintaining software systems~\cite{sommerville2011software}.

\textbf{Data Preprocessing:}  
A set of techniques applied to raw data to improve quality, consistency, and suitability for machine learning models~\cite{mcCallum1998comparison}.

\textbf{Tokenization:}  
The process of splitting text into smaller units such as words or tokens for further analysis~\cite{aibook2006}.

\textbf{Stop Words:}  
Commonly occurring words (e.g., ``the'', ``is'') that are often removed during text preprocessing to reduce noise~\cite{mcCallum1998comparison}.

\textbf{Lemmatization:}  
The process of reducing words to their base or dictionary form to normalize textual data~\cite{aibook2006}.

\textbf{Bag-of-Words (BoW):}  
A text representation technique that converts text into a fixed-length vector by counting word occurrences while ignoring word order~\cite{mcCallum1998comparison}.

\textbf{Term Frequency-Inverse Document Frequency (TF-IDF):}  
A statistical measure that evaluates the importance of a word in a document relative to a collection of documents~\cite{mcCallum1998comparison}.

\textbf{Ensemble Aggregation:}  
The process of combining outputs from multiple machine learning models using methods such as majority voting or weighted averaging to produce a final prediction.

\textbf{Token-Level Explanation:}  
An explainability approach that identifies and highlights the contribution of individual words or tokens to a model's prediction.

\textbf{Inference:}  
The process of using a trained machine learning model to generate predictions on previously unseen data.

\textbf{Precision:}  
A performance metric that measures the proportion of correctly predicted positive instances among all predicted positives~\cite{aibook2006}.

\textbf{Recall:}  
A performance metric that measures the proportion of actual positive instances correctly identified by the model~\cite{aibook2006}.

\textbf{F1-Score:}  
The harmonic mean of precision and recall, used to evaluate overall classification performance~\cite{aibook2006}.

\textbf{Confusion Matrix:}  
A tabular representation that summarizes the performance of a classification model by comparing predicted and actual class labels~\cite{aibook2006}.

\textbf{Model Explainability:}  
The ability to understand, interpret, and justify how a machine learning model arrives at its predictions~\cite{ribeiro2016why}.

\textbf{Deep Learning:}  
A subset of machine learning that uses multi-layered neural networks to model complex and hierarchical patterns in data~\cite{aibook2014}.

\textbf{Class Imbalance:}  
A condition in which some classes in a dataset are significantly underrepresented compared to others, potentially biasing model performance.
% -----------------------------------------------------------------------------------------------